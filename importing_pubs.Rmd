---
title: "Adding Neotoma Publications"
author:
- affiliation: University of Wisconsin - Madison
  name: Simon Goring
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  word_document: default
  pdf_document:
    toc: yes
  html_document:
    code_folding: show
    collapsed: yes
    highlight: pygments
    smooth_scroll: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
tags:
- SQL
- Neotoma
- CrossRef
- DOI
- citation
- bibJSON
abstract: |
  This document details the process of pulling title strings from the Neotoma Database (using a MySQL dump of the database) and pushing the parsed titles into a large table.  This parsed table is then used to pull close matches from the CrossRef API.
---

```{r, echo = FALSE, warning = FALSE}
library(pander)
library(rmarkdown)
library(knitr)

opts_chunk$set(tidy.opts = list(width.cutoff = 60),
               tidy = TRUE)
```

# Introduction

For many Neotoma datasets, the Database has limited information in the `publication` table as a result of legacy decisions, mostly a result of imports from other data aggregations.  Publication information is stored as a wide table, but many records only contain a `publicationid` key, year of publication and the full text citation, along with contactids for the main authors.  To help support ongoing data citation and best-practices in the management of data resources it would be best if Neotoma could support the export of data using data standards that are widely accepted and supported.  Both [DublinCore] and the `bibJSON` [REF] format are well accepted formats that are supported by clear documentation.  Publishers, in addition, support data access through the CrossRef API.  The [CrossRef API is well documented](ref), and contains a number of key fields.  Thus, the current challenge for Neotoma is to manage our references in such a way that researchers can easily access data associated with dataset-related publciations, that we can place constraints on our publication tables to assure content quality is maintained in the upload process, and that data is returned through the API in such a way that it conforms to both researcher expectations and best practices for machine-assisted data harvesting.

# Current Data Format

The publication information in Neotoma is spread across several tables within the database (Figure 1).  The publication table contains information strictly about the publication and contains no information about individuals.  This information, along with author and editor order, is contained in the `publicationauthors` and `publicationeditors` tables.  The `contactid` field is a foreign key that links to the primary key of the `publicationauthors` and `publicationeditors` tables.  In practice the fields in `contacts` are not always filled.

<figure>
<img src="images/pub_relations.svg">
<figcaption><b>Figure 1</b>. Neotoma Paleoecological Database publication relationships.  Foreign keys within the database are indicated using hashed lines, primary keys are contained within light blue boxes.</figshare>

```{r, echo = FALSE, results = 'asis', messages=FALSE, warnings=FALSE}

library(RODBC, quietly = TRUE, verbose = FALSE)

con <- odbcConnect("Neotoma", uid="SA", pwd="c@mpf1re")

con <- odbcDriverConnect('Driver={ODBC Driver 11 for SQL Server};Server=localhost;Database=Neotoma;Uid=SA;Pwd=c@mpf1re;')

library(RMySQL, quietly = TRUE, verbose = FALSE)

mydb <- dbConnect(MySQL(), user = 'root', password = 'c@mpf1re', 
                  dbname = 'neotoma', host = 'localhost')

publications <- dbGetQuery(mydb, "select * from publications")

pander::pandoc.table(publications[1,c(1, 3, 4, 5)], table.split.cells = 8,
                     use.hyphening = TRUE, split.table = 100)

```

Of all the `r nrow(publications)` records, `r sum(is.na(publications$DOI))` have no existing DOI.  Additionally, `r sum(is.na(publications$ArticleTitle))` have no reliable title (except as part of the `citation` field).  No record (as far as I can tell) contains the full, rich data needed for bibJSON output. 

# bibJSON Briefly

A [fuller definition of the bibJSON standard](http://okfnlabs.org/bibjson/) is available online, but the idea is for a light & portable standard to represent bibliographic metadata using JSON (or JSON-LD).  While bibJSON uses a default set of keys defined for `BibTeX`, part of bibJSON's beauty is that it has no defined namespace limits, so if we decide, as a community, to add extra data to the format (like the inclusion of IGSNs), then we can do that as well without breaking the standard.

## The bibJSON Schema

The bibJSON format looks something like this (from [http://okfnlabs.org/bibjson/]()):

```json

{
    "title": "Open Bibliography for Science, Technology and Medicine",
    "author":[
        {"name": "Richard Jones"},
        {"name": "Mark MacGillivray"},
        {"name": "Peter Murray-Rust"},
        {"name": "Jim Pitman"},
        {"name": "Peter Sefton"},
        {"name": "Ben O'Steen"},
        {"name": "William Waites"}
    ],
    "type": "article",
    "year": "2011",
    "journal": {"name": "Journal of Cheminformatics"},
    "link": [{"url":"http://www.jcheminf.com/content/3/1/47"}],
    "identifier": [{"type":"doi","id":"10.1186/1758-2946-3-47"}]
}
```

`bibJSON` is intended to map onto bibtex to provide ease of use for information transfer and re-use.  The standard can also  provide arich set of information that can then be ingested into any number of other data resources that use JSON-LD formats, including [codemeta], [LiPD], and others.

## How well does Neotoma Map onto bibJSON?

This is a fairly naive attempt to map on to bibJSON.  I could try harder, but I think the point is made here. Let's assume we're using a Beringian paper as the test object.  The paper is:

> Anderson, P.M., and A.V. Lozhkin. 2001. The latest Pleistocene interstade (Karginskii/Boutellier interval) of Beringia:   variations in paleoenvironments and implications for paleoclimatic interpretations. *Quaternary Science Reviews*, **20**: 93-125.

The paper has a `PublicationID` of 848 in the Neotoma Database:

```{r}
library(jsonlite)

bibJSON_test <- toJSON(list(title =  dbGetQuery(mydb, "select ArticleTitle from publications 
                                                where PublicationID=848"),
                     author = dbGetQuery(mydb, "select concat(LeadingInitials, ', ', FamilyName) as name from contacts
                                         where ContactID IN (select ContactID from publicationauthors where
                                         PublicationID=848)"),
                     type = dbGetQuery(mydb, "select ArticleTitle from publications where PublicationID=848"),
                     year = dbGetQuery(mydb, "select Year from publications where PublicationID=848"),
                     journal = dbGetQuery(mydb, "select Journal from publications where PublicationID=848"),
                     link = dbGetQuery(mydb, "select URL from publications where PublicationID=848"),
                     identifier = dbGetQuery(mydb, "select DOI from publications where PublicationID=848")))
```

```{r echo = FALSE, results = 'asis'}
bibJSON_test
```

For many records even basic metadata is missing.  There are `r nrow(dbGetQuery(mydb, "select * from publications where ArticleTitle IS NULL"))` records out of `r nrow(dbGetQuery(mydb, "select * from publications"))` that are missing the article title.  This isn't the end of the world, but we want to start making a crack at fixing it.

# The CrossRef Metadata API

Luckily, we can make use of the [CrossRef Metadata API](http://labs.crossref.org/crossref-metadata-search/), which can return document metadata from DOIs, and we can use this to fill in our records.  The CrossRef Metadata for the paper with DOI `10.1016/j.yqres.2009.05.006` (Eric's conventional bulk sediment error paper) looks something like this:

```json
{"status":"ok",
  "message-type":"work",
  "message-version":"1.0.0",
  "message":{"indexed":{"date-parts":[[2015,12,22]],
                        "date-time":"2015-12-22T11:07:51Z",
                        "timestamp":1450782471165},
             "reference-count":44,
             "publisher":"Elsevier BV",
             "issue":"2",
             "license":[{"content-version":"tdm",
                         "delay-in-days":0,
                         "start":{"date-parts":[[2009,9,1]],
                                  "date-time":"2009-09-01T00:00:00Z",
                                  "timestamp":1251763200000},
                         "URL":"http:\/\/www.elsevier.com\/tdm\/userlicense\/1.0\/"}],
             "published-print":{"date-parts":[[2009,9]]},
             "DOI":"10.1016\/j.yqres.2009.05.006",
             "type":"journal-article",
             "created":{"date-parts":[[2009,7,10]],
                        "date-time":"2009-07-10T09:40:02Z",
                        "timestamp":1247218802000},
             "page":"301-308",
             "source":"CrossRef",
             "title":["The magnitude of error in conventional bulk-sediment radiocarbon dates from central North America"],
             "prefix":"http:\/\/id.crossref.org\/prefix\/10.1016",
             "volume":"72",
             "author":[{"affiliation":[],"family":"Grimm","given":"Eric C."},
                       {"affiliation":[],"family":"Maher","given":"Louis J.","suffix":"Jr."},
                       {"affiliation":[],"family":"Nelson","given":"David M."}],
             "member":"http:\/\/id.crossref.org\/member\/78",
             "container-title":["Quaternary Research"],
             "link":[{"intended-application":"text-mining",
                      "content-version":"vor",
                      "content-type":"text\/xml",
                      "URL":"http:\/\/api.elsevier.com\/content\/article\/PII:S003358940900057X?httpAccept=text\/xml"},
                     {"intended-application":"text-mining",
                      "content-version":"vor",
                      "content-type":"text\/plain",
                      "URL":"http:\/\/api.elsevier.com\/content\/article\/PII:S003358940900057X?httpAccept=text\/plain"}],
             "deposited":{"date-parts":[[2015,12,19]],
                          "date-time":"2015-12-19T21:05:03Z",
                          "timestamp":1450559103000},
             "score":1.0,
             "subtitle":[],
             "issued":{"date-parts":[[2009,9]]},
             "alternative-id":["S003358940900057X"],
             "URL":"http:\/\/dx.doi.org\/10.1016\/j.yqres.2009.05.006",
             "ISSN":["0033-5894"],
             "subject":["Earth-Surface Processes","Earth and Planetary Sciences(all)"]}}
```

So . . . pretty rich.

We need to be able to pull out, at minimum information about article `"type"`, a link (`"created":"URL"`) & identifier (`"DOI"`), author names (`"created":"author"`), year of publication (`"created":"date-parts"`) & journal (`"created":"container-title"`).  Given that I've spelled out the location for each of these in the CrossRef JSON schema it's likely that we'll be able to pull those in for articles contained in CrossRef (presuming we can find them).

# Getting Started - Neotoma

To pull out bibliographic information from Neotoma we can try parsing individual text-string citations from Neotoma using regular expressions, or use a hybrid method, that both parses title or text strings from the text-string Neotoma citations and then makes use of of the CrossRef and JSTOR APIs to obtain more information about the individual records.

This workflow then looks like this:

1. Load in the Neotoma publication table
2. For records without "Title" fields or DOIs (but with full citations):
  a. Use regular Expression matching to extract the likely "Title" field
  b. Pass the string to the CrossRef API to get the five closest matches
  c. All 5 matches go into a list called `results_list`
  d. Closest match & DOI go into `results_frame` along with an estimate of distance.
  
To start, we create the variables (or load them if we've done partial runs earlier):

```{r}
library(devtools)
library(rcrossref)
library(jsonlite)
library(stringr)
library(stringdist)

#  Some of this analysis is time consuming.  To save time in multiple runs we
#  save as we go (see below), so the first thing we do is look for evidence
#  that a past run has occurred.

publications <- dbGetQuery(mydb, "select * from publications")

if ('results_frame.csv' %in% list.files()) {
  results_frame <- read.csv('results_frame.csv')
  results_list <- readRDS('results_list.RDS')
} else {
  results_frame <- data.frame(hits       = rep(NA, nrow(publications)),
                              query      = rep(NA, nrow(publications)),
                              best_title = rep(NA, nrow(publications)),
                              proximity  = rep(NA, nrow(publications)),
                              prox_pct   = rep(NA, nrow(publications)),
                              best_doi   = rep(NA, nrow(publications)))
  results_list <- list()
  
  publications$Modified <- NA
}

```

We've now done a few things.  One, I wrote the code so that it's ultimately going to spit out a table called `results_frame`, and, if that table exists, it will load it (so we can avoid running through records we've already checked).  Then, for each record we use the [`rcrossref` package](https://github.com/ropensci/rcrossref)'s `cr_works_` function (which returns results in JSON format) to find the closest match.

There's a few other things that the code below does, in part because of the particulars of the `publications` table.  For example, we pull the title out of the full citation string by searching for periods in the text string.  We use regular expresson matching to match `'(18|19|20)\\d{2}\\. '` which should catch anything with the form `'1800. '`, basically, a year in the 19^th^, 20^th^ or 21^st^ century, followed by a period and a space.

The title (in general) should be bound by the date and the next period.  In practice this doesn't exactly work, things like `U.S.A.` get truncated. To fix this we need to generate a more complex regular expression search (further down in the code block, where we use the `str_locate_all` block. Given the more complex search we get a long enough query that we can match a record in CrossRef.  Once we get the match we can pull the DOI metadata and fill the fields properly.

There are still some problems, mostly date issues.  For example, there are some papers listed as `n.d.`, or with no trailling period in the age field.  In these cases I'm going to let them pass on as `NA` records.  They ultimately need more looking into, so it's okay to let them screw up for now.

```{r, eval = FALSE}
#  For each record:
for (i in 1:nrow(publications)) {
  # Only run for records without DOIs or filled results:
  if (is.na(publications$DOI[i]) & 
     is.na(results_frame$hits[i])) {
    # If we need to strip out the title from the paper citation (it hasn't been done yet in Neotoma):
    if (is.na(publications$ArticleTitle[i])) {
      ## We want to pull a title in from the full citation:
      year_text <- str_locate(publications$Citation[i], '(18|19|20)\\d{2}[a-zA-Z]{0,1}\\. ')
      
      # The regular expression string here searches for all periods, except those preceeded by
      # a St (shortening "Saint"), or preeceding an "A." (so we don't have to worry about the 'S').
      # If the U.S.A is at the end of a title we can't ignore the period, but we can if it is followed by
      # a comma or a lowercase letter.
      periods   <- str_locate_all(publications$Citation[i], '(?<!(U|St))\\.(?!((A\\.)|,|:|( [a-z])))')[[1]]
      
      if (!any(is.na(year_text)) & max(year_text) < max(periods)) {
        # The date has valid information, then get the title (or at least part of the title between
        # a set of paired periods).
        # This chokes on things like "U.S.", for now that's a small subset.
        
        title <- substr(publications$Citation[i],
                        start = year_text[2] + 1,
                        stop  = min(unlist(periods)[unlist(periods) > max(year_text)]))
        year  <-  substr(publications$Citation[i],
                        start = year_text[1],
                        stop  = year_text[2] - 2)
        
        # I've added a flag for the base publication table so I can differentiate between
        # publications that have had their titles entered by hand, and those 
        # I've done programmatically.
        publications$Year[i]         <- year
        publications$ArticleTitle[i] <- title
        publications$Modified[i]     <- TRUE
      } else {
        # If there's no apparent date field.  Works out to ~ 210 records.
        title <- NA
        year <- NA
      }
    } else {
      ## If the ArticleTitle is there:
      title <- publications$ArticleTitle[i]
      year  <- publications$Year[i]
    }
  
    # If we were able to recover a title to use in the search:
    if (!is.na(title)) {
      # and if there's a year assigned:
      if (!is.na(as.numeric(year))) {
        # This adds a filter to clean up the references, so that we're only pulling the closest set:
        year_pub <- paste0(year, c('-01-01', '-12-31'))
        records <- jsonlite::fromJSON(cr_works_(query = title, 
                                              filter = c(from_print_pub_date = year_pub[1],
                                                            until_pub_date   = year_pub[2]),
                                              limit = 5))
      } else {
        # If there's no year assigned we can't limit the search:
        records <- jsonlite::fromJSON(cr_works_(query = title, 
                                                limit = 5))
      }
    
      # We've created a search that looks for the title string, and limits by
      # publication date.  Because we're only looking for very close matches
      # we're just going to take the closest
  
      if (records$status == 'ok' & !is.null(records$message$items$title)) {
        # How many insertions or deletions are needed to find the most similar title:
        # for example:  aaa to bbb should give 3, while aba to bbb gives 2.
        # In general we should suspect anything over ~10 or so. . . 
        
        edit_dists <- sapply(records$message$items$title, function(x)adist(tolower(x[1]), tolower(title)))
        matched_title <- which.min(edit_dists)
        
        if (!all(sapply(records$message$items$title, length) == 0)) {
          # We have a good record, now lets extract the information:
          results_frame$hits[i]       <- records$message$`total-results`
          results_frame$query[i]      <- records$message$query$`search-terms`
          results_frame$proximity[i]  <- edit_dists[matched_title]
          results_frame$prox_pct[i]   <- edit_dists[matched_title] / nchar(
            records$message$items$title[[matched_title]])
          results_frame$best_title[i] <- records$message$items$title[[matched_title]]
          results_frame$best_doi[i]   <- records$message$items$DOI[[matched_title]]
        
          # We're storing this so that if there are bad matches we can see if it's because of the
          #  distance measure we're using or not.
          results_list[[i]] <- unlist(records$message$items$title)
        }
      }
    }
    write.csv(results_frame, 'results_frame.csv', row.names = FALSE)
    write.csv(publications, 'pubs_edited.csv', row.names = FALSE)
    saveRDS(results_list, 'results_list.RDS')
  }
  else if (!is.na(publications$DOI[i]) & 
     is.na(results_frame$hits[i])) {
    # If there's already a record, but it's not in the `results_frame`:
    results_frame$hits[i]     <- NA
    results_frame$query[i]    <- publications$ArticleTitle[i]
    results_frame$best_doi[i] <- publications$DOI[i]
  }
}

```

We're putting the output of the CrossRef API into a couple of `csv` files for readability & hand editing.  The `pubs_edited.csv` file is the publications table.  The `results_frame.csv` has more content, it includes:

* `hits`: The number of matching references found in CrossRef
* `query`: Equivalent to the `ArticleTitle` in the Publications table.
* `best_title`: The title CrossRef returns as the "best match".
* `proximity`: The number of edits required to convert one title to the next (*e.g.*, From "hat" to "hit" is 1 edit, from "hat" to "horse" is 4, from "Up" to "up" is 1).
* `prox_pct`: The number of edits as a function of the title length.
* `best_doi`: The DOI associated with the best match.

We also save the list of titles returned in the list `results_list`, just to check that "wrong" titles aren't getting picked over titles that seem more correct.  We need to figure out a way to check these more reliably.

The regular expression searching works well in general.  Most article titles are of reasonable length and appear to match well.  This is a rough representation, but we see in general that titles are about as long as the full citation string, but somewhat shorter.  There are a bunch of results with a length of 2.  These are results for which the query is "NA".  I'm not entirely sure why that's happening.

```{r title_lengths, echo=FALSE, warning=FALSE}
library(ggplot2)

ggplot(data.frame(x = nchar(as.character(publications$Citation)), 
                  y = nchar(as.character(results_frame$query)))) +
  geom_point(aes(x = x, y = y)) +
  coord_cartesian(xlim = c(0, 500), ylim = c(0, 275), expand = FALSE) +
  xlab('Citation length (char)') +
  ylab('Title Length (char)') +
  geom_abline(slope = 1, intercept = 0) +
  theme_light()
```

From this, we need to the go and identify which articles are good matches, by hand.  This is work that was done by myself and Analise Lindborg.  The document is up on [Google Drive](https://docs.google.com/spreadsheets/d/10_u41Q3Vd9cvyrBS9RiL9g5tCaI3Cimpry7k1iwd-ug/edit?usp=sharing), and contains several columns including an additional column for Notes, about impartial matches or errors in the original publication information.

# Reinsertion

```{r}

pubs <- neotoma::get_table('publications')

library(googlesheets)

pubs_gs <- readr::read_csv('marked_dois.csv')

good_pubs <- pubs_gs %>% filter(`Match OK` %in% "Yes")

```

This the provides us with `r nrow(good_pubs)` records with listed DOIs.  Of these, `r sum(good_pubs$PublicationID %in% pubs$PublicationID[!is.na(pubs$DOI)])` had existing DOIs.  This represents a major increase in the number of linked records within Neotoma.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(rcrossref)

if ('filled_data.rds' %in% list.files('output')) {
  readRDS('output/filled_data.rds')
} else {
  filled_data <- rcrossref::cr_cn(dois = good_pubs$`CrossRef DOI`, 
                                format = 'citeproc-json', .progress = 'none')
  saveRDS(filled_data, 'output/filled_data.rds')  
}

list_names <- unique(unlist(sapply(filled_data, names)))
each_names <- bind_rows(lapply(filled_data, function(x) data.frame(list_names %in% names(x))))

```

The DOI tags return `r length(unique(unlist(sapply(filled_data, '[[', 'type'))))` different document types.  These include `r paste0(unique(unlist(sapply(filled_data, '[[', 'type'))), collapse = ", ")`.  The full set of fields includes:

```{r}

knitr::kable(as.data.frame(table(list_names)))

```

```r
for (i in 1:nrow(good_pubs)) {
  
  pub_match <- match(good_pubs$PublicationID[i], pubs$PublicationID)
  
  pubs$Publisher[pub_match] <- filled_data[[i]]$publisher
  pubs$DOI[pub_match]       <- filled_data[[i]]$DOI
  pubs$URL[pub_match]       <- filled_data[[i]]$URL
  pubs$page[pub_match]      <- filled_data[[i]]$Pages
  pubs$Year[pub_match]      <- filled_data[[i]]$`published-print`$`date-parts`[1]
  
  # There are several different 'types' in CrossRef.  
  # Our current pull shows "book", "book-chapter", "journal-article", "monograph", "report"
  if (filled_data[[i]]$type == 'monograph') {
    
    pubs$BookTitle[pub_match] <- filled_data[[i]]$`title`
    pubs$PubTypeID[pub_match] <- 3
    pubs$Year[pub_match]      <- filled_data[[i]]$`published-print`$`date-parts`[1]
    
  }
  
  if (filled_data[[i]]$type == 'book') {
    if (is.null(filled_data[[i]]$`container-title`)) {
      # This is a book, it has no "container"
      pubs$BookTitle[pub_match] <- filled_data[[i]]$`title`
      pubs$PubTypeID[pub_match] <- 3
      pubs$Year[pub_match]      <- filled_data[[i]]$`published-print`$`date-parts`[1]
    } else {
      # This has a container:
      pubs$ArticleTitle[pub_match] <- filled_data[[i]]$`title`
      pubs$PubTypeID[pub_match] <- 2
      pubs$Year[pub_match]      <- filled_data[[i]]$`published-print`$`date-parts`[1]
    }
    
  }
  
  if (filled_data[[i]] == 'journal-article') {
    pubs$Issue[pub_match]     <- filled_data[[i]]$issue
    
  }
  
  if (filled_data[[i]]$type == 'report') {
    if (is.null(filled_data[[i]]$`container-title`)) {
      pubs$PubTypeID[pub_match] <- 3
      pubs$BookTitle[pub_match] <- filled_data[[i]]$`title`
    } else {
      pubs$PubTypeID[pub_match] <- 3
    }
  }
  
  if (filled_data[[i]]$type == 'report') {
  
  
  
}

```
